{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "## Homework 4\n",
    "\n",
    "_October 6th, 2020_\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "(Beck Exercise 5.2) Consider the Freudenstein and Roth test function\n",
    "\n",
    "$$f(x) = f_1(x)^2 + f_2(x)^2, \\ \\ x \\in \\mathbb R^2$$\n",
    "\n",
    "where\n",
    "$$f_1(x) = -13 + x_1 + ((5-x_2)x_2-2)x_2$$\n",
    "\n",
    "$$f_2(x) = -29 + x_1 + ((1+x_2)x_2-14)x_2$$\n",
    "\n",
    "**(i)** Show that the function $f$ has three stationary points. Find them and prove that one is a global minimizer, one is a strict local minimum, and the third is a saddle point.\n",
    "\n",
    "(see HW4.pdf for this work. Some of the intermediate calculations required are below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from numpy.linalg import cholesky\n",
    "from numpy.linalg import solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show_err=false; \n",
       "function code_toggle_err() {\n",
       " if (code_show_err){\n",
       " $('div.output_stderr').hide();\n",
       " } else {\n",
       " $('div.output_stderr').show();\n",
       " }\n",
       " code_show_err = !code_show_err\n",
       "} \n",
       "$( document ).ready(code_toggle_err);\n",
       "</script>\n",
       "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show_err=false; \n",
    "function code_toggle_err() {\n",
    " if (code_show_err){\n",
    " $('div.output_stderr').hide();\n",
    " } else {\n",
    " $('div.output_stderr').show();\n",
    " }\n",
    " code_show_err = !code_show_err\n",
    "} \n",
    "$( document ).ready(code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective function\n",
    "f1 = lambda x: -13 + x[0] + ((5 - x[1])*x[1] - 2)*x[1]\n",
    "f2 = lambda x: -29 + x[0] + ((1 + x[1])*x[1] - 14)*x[1]\n",
    "\n",
    "fvec = lambda x: np.array([f1(x),f2(x)])\n",
    "\n",
    "f = lambda x: fvec(x).T@fvec(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient\n",
    "g = lambda x: np.array([-84 + 4*x[0] + 12*x[1]**2 - 32*x[1],\n",
    "                        864 - 32*x[0] + 24*x[1] + 24*x[0]*x[1] - 240*x[1]**2 + 8*x[1]**3 - 40*x[1]**4 + 12*x[1]**5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient at x1:  [0 0] function value at x2:  0\n"
     ]
    }
   ],
   "source": [
    "# global minimum\n",
    "x1 = np.array([5,4])\n",
    "print(\"gradient at x1: \", g(x1), \"function value at x2: \", f(x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient at x2:  [-3.55271368e-15  1.33226763e-14] function value at x2:  48.98425367924004\n"
     ]
    }
   ],
   "source": [
    "# local minimum\n",
    "x2 = np.array([(1/3)*(53-4*np.sqrt(22)), (1/3)*(2-np.sqrt(22))])\n",
    "print(\"gradient at x2: \", g(x2), \"function value at x2: \", f(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient at x3:  [0. 0.] function value at x3:  819.01025935231\n"
     ]
    }
   ],
   "source": [
    "# saddle point\n",
    "x3 = np.array([(1/3)*(53+4*np.sqrt(22)), (1/3)*(2+np.sqrt(22))])\n",
    "print(\"gradient at x3: \", g(x3), \"function value at x3: \", f(x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hessian\n",
    "h = lambda x: np.array([[4, 24*x[1]-32],\n",
    "                        [24*x[1]-32 ,24 + 24*x[0] - 480*x[1] + 24*x[1]**2 - 160*x[1]**3 + 60*x[1]**4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   4,   64],\n",
       "       [  64, 3728]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.        , -53.52332608],\n",
       "       [-53.52332608, 901.88775184]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   4.        ,   21.52332608],\n",
       "       [  21.52332608, -643.51738147]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(ii)** Employ the following three methods on the problem of minimizing f. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Method with backtracking\n",
    "**(a)** Gradient Method with backtracking and parameters $(s, \\alpha, \\beta) = (1, 0.5, 0.5)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_method_backtracking(f,g,x0,s,alpha,beta,epsilon):\n",
    "    \"\"\"\n",
    "    Gradient method with backtracking stepsize rule\n",
    "    INPUT\n",
    "    =======================================\n",
    "    f ......... objective function\n",
    "    g ......... gradient of the objective function\n",
    "    x0......... initial point\n",
    "    s ......... initial choice of stepsize\n",
    "    alpha ..... tolerance parameter for the stepsize selection\n",
    "    beta ...... the constant in which the stepsize is multiplied \n",
    "                at each backtracking step (0<beta<1)\n",
    "    epsilon ... tolerance parameter for stopping rule\n",
    "    OUTPUT\n",
    "    =======================================\n",
    "    x ......... optimal solution (up to a tolerance) \n",
    "                of min f(x)\n",
    "    fun_val ... optimal function value\n",
    "    \"\"\"\n",
    "    x=x0\n",
    "    grad=g(x)\n",
    "    fun_val=f(x)\n",
    "    iter=0\n",
    "    while (norm(grad)>epsilon):\n",
    "        iter=iter+1\n",
    "        t=s\n",
    "        while (fun_val-f(x-t*grad)<alpha*t*norm(grad)**2):\n",
    "            t=beta*t\n",
    "        x=x-t*grad\n",
    "        fun_val=f(x)\n",
    "        grad=g(x)\n",
    "    print('iter_number = '+ str(iter) + ' norm_grad = ' + str(norm(grad)) + ' fun_val = ' + str(fun_val))\n",
    "    return x,fun_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "s = 1\n",
    "alpha = 0.5 \n",
    "beta = 0.5\n",
    "epsilon = 10e-5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saeth\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in long_scalars\n",
      "  \n",
      "C:\\Users\\saeth\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in long_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_number = 1965 norm_grad = 9.980272066889434e-05 fun_val = 1.689658077800942e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([4.99996587, 4.00000058]), 1.689658077800942e-09)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starting point\n",
    "x0 = np.array([-50,7])\n",
    "gradient_method_backtracking(f,g,x0,s,alpha,beta,epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting at $(-50,7)$ it took 1965 iterations to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saeth\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in long_scalars\n",
      "  \n",
      "C:\\Users\\saeth\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in long_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_number = 2155 norm_grad = 9.930646621307876e-05 fun_val = 1.6399482633305737e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.00003362, 3.99999943]), 1.6399482633305737e-09)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starting point\n",
    "x0 = np.array([20,7])\n",
    "gradient_method_backtracking(f,g,x0,s,alpha,beta,epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting at $(20,7)$ it took 2155 iterations to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saeth\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in long_scalars\n",
      "  \n",
      "C:\\Users\\saeth\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in long_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_number = 2117 norm_grad = 9.717228016084474e-05 fun_val = 48.98425368488135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([11.41289603, -0.89679829]), 48.98425368488135)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starting point\n",
    "x0 = np.array([20,-18])\n",
    "gradient_method_backtracking(f,g,x0,s,alpha,beta,epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting at $(20,-18)$ it took 2117 iterations to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saeth\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in long_scalars\n",
      "  \n",
      "C:\\Users\\saeth\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in long_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_number = 1832 norm_grad = 9.553119633971772e-05 fun_val = 1.5451971987425595e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.00003264, 3.99999944]), 1.5451971987425595e-09)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starting point\n",
    "x0 = np.array([5,-10])\n",
    "gradient_method_backtracking(f,g,x0,s,alpha,beta,epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting at $(5,-10)$ it took 1832 iterations to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Newton's Method\n",
    "**(b)** Hybrid Newton's Method with $(\\alpha, \\beta) = (0.5,0.5)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_newton(f,g,x0,h,alpha,beta,epsilon):\n",
    "    \"\"\"\n",
    "    Hybrid Newton's method\n",
    "    INPUT\n",
    "    =======================================\n",
    "    f ......... objective function\n",
    "    g ......... gradient of the objective function\n",
    "    h ......... hessian of the objective function\n",
    "    x0......... initial point\n",
    "    alpha ..... tolerance parameter for the stepsize selection\n",
    "    beta ...... the constant in which the stepsize is multiplied \n",
    "                at each backtracking step (0<beta<1)\n",
    "    epsilon ... tolerance parameter for stopping rule\n",
    "    OUTPUT\n",
    "    =======================================\n",
    "    x ......... optimal solution (up to a tolerance) \n",
    "                of min f(x)\n",
    "    fun_val ... optimal function value\n",
    "    \"\"\"\n",
    "    x=x0\n",
    "    grad=g(x)\n",
    "    hval = h(x)\n",
    "    \n",
    "    try:\n",
    "        L = cholesky(hval)\n",
    "        d = solve(L.T, solve(L, grad))\n",
    "    except:\n",
    "        d = grad\n",
    "    \n",
    "    fun_val=f(x)\n",
    "    iter=0\n",
    "    while (norm(grad)>epsilon) & (iter<10000):\n",
    "        iter=iter+1\n",
    "        t=1\n",
    "        while (f(x-t*d) > f(x) - alpha*t*grad.T@d):\n",
    "            t=beta*t\n",
    "        x=x-t*d\n",
    "        fun_val=f(x)\n",
    "        grad=g(x)\n",
    "        hval = h(x)\n",
    "        try:\n",
    "            L = cholesky(hval)\n",
    "            d = solve(L.T, solve(L, grad))\n",
    "        except:\n",
    "            d = grad\n",
    "            \n",
    "    if(iter == 10000):\n",
    "        print(\"method did not converge\")\n",
    "        \n",
    "    print('iter_number = '+ str(iter) + ' norm_grad = ' + str(norm(grad)) + ' fun_val = ' + str(fun_val))\n",
    "    \n",
    "    return x,fun_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_number = 8 norm_grad = 4.403801190986987e-09 fun_val = 3.218858347296457e-21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5., 4.]), 3.218858347296457e-21)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.array([-50,7])\n",
    "hybrid_newton(f,g,x0,h,alpha,beta,epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting at $(-50,7)$ it took 8 iterations to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_number = 8 norm_grad = 6.753950355311321e-09 fun_val = 7.573184445518013e-21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5., 4.]), 7.573184445518013e-21)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.array([20,7])\n",
    "hybrid_newton(f,g,x0,h,alpha,beta,epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting at $(20,7)$ it took 8 iterations to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_number = 15 norm_grad = 5.948523474557842e-05 fun_val = 48.98425367924293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([11.41277747, -0.89680541]), 48.98425367924293)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.array([20,-18])\n",
    "hybrid_newton(f,g,x0,h,alpha,beta,epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting at $(20,-18)$ it took 15 iterations to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_number = 13 norm_grad = 4.285820184238037e-08 fun_val = 48.98425367924004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([11.41277899, -0.89680525]), 48.98425367924004)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.array([5,-10])\n",
    "hybrid_newton(f,g,x0,h,alpha,beta,epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting at $(5,-10)$ it took 13 iterations to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Damped Gauss-Newton Method\n",
    "**(c)**  Damped Gauss-Newton Method with backtracking line search strategy with $(s,\\alpha, \\beta) = (1,0.5,0.5)$ \n",
    "\n",
    "The Gauss-Newton Method is essentially  a scaled gradient method with the following positive definite-scaling matrix\n",
    "\n",
    "$$D_k = \\frac{1}{2}(J(x_k)^TJ(x_k))^{-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_gradient_method_backtracking(J,f,g,x0,s,alpha,beta,epsilon):\n",
    "    \"\"\"\n",
    "    Scaled gradient method with backtracking stepsize rule\n",
    "    INPUT\n",
    "    =======================================\n",
    "    J ......... scaling matrix\n",
    "    f ......... objective function\n",
    "    g ......... gradient of the objective function\n",
    "    x0......... initial point\n",
    "    s ......... initial choice of stepsize\n",
    "    alpha ..... tolerance parameter for the stepsize selection\n",
    "    beta ...... the constant in which the stepsize is multiplied \n",
    "                at each backtracking step (0<beta<1)\n",
    "    epsilon ... tolerance parameter for stopping rule\n",
    "    OUTPUT\n",
    "    =======================================\n",
    "    x ......... optimal solution (up to a tolerance) \n",
    "                of min f(x)\n",
    "    fun_val ... optimal function value\n",
    "    \"\"\"\n",
    "    x=x0\n",
    "    grad=g(x)\n",
    "    fun_val=f(x)\n",
    "    D = (1/2)*np.linalg.inv(J(x).T@J(x))\n",
    "    iter=0\n",
    "    while ((norm(grad)>epsilon)&(iter<10000)):\n",
    "        iter=iter+1\n",
    "        t=s\n",
    "        while (fun_val-f(x-t*D@grad)<alpha*t*grad@D@grad):\n",
    "            t=beta*t\n",
    "        x=x-t*D@grad\n",
    "        fun_val=f(x)\n",
    "        grad=g(x)\n",
    "        D = (1/2)*np.linalg.inv(J(x).T@J(x))\n",
    "    if (iter==10000):\n",
    "        print('did not converge')\n",
    "    print('iter_number = '+ str(iter) + ' norm_grad = ' + str(norm(grad)) + ' fun_val = ' + str(fun_val))\n",
    "    return x,fun_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jacobian\n",
    "J = lambda x: np.array([[1, 10 - 27*x[1] + 10*x[1]**2 - x[1]**3],\n",
    "                        [1, -14 - 13*x[1] + 2*x[1]**2 + x[1]**3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_number = 21 norm_grad = 2.003819082630959e-05 fun_val = 4.47489664292633e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.00000175, 3.99999996]), 4.47489664292633e-12)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.array([-50,7])\n",
    "scaled_gradient_method_backtracking(J,f,g,x0,s,alpha,beta,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_number = 22 norm_grad = 8.265639256302167e-05 fun_val = 1.00716430311037e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.00000025, 4.00000002]), 1.00716430311037e-12)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.array([20,7])\n",
    "scaled_gradient_method_backtracking(J,f,g,x0,s,alpha,beta,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_number = 63 norm_grad = 7.13070672683346e-05 fun_val = 3.5420750664235985e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.0000014 , 3.99999996]), 3.5420750664235985e-12)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.array([20,18])\n",
    "scaled_gradient_method_backtracking(J,f,g,x0,s,alpha,beta,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_number = 879 norm_grad = 9.82011484157245e-05 fun_val = 48.98425367938374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([11.41276065, -0.89680645]), 48.98425367938374)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.array([5,-10])\n",
    "scaled_gradient_method_backtracking(J,f,g,x0,s,alpha,beta,epsilon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
